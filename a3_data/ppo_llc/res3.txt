========== LunarLanderContinuous-v2 ==========
Seed: 1405775871
Loading hyperparameters from: hyperparams/ppo.yml
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('batch_size', 64),
             ('ent_coef', 0.01),
             ('gae_lambda', 0.98),
             ('gamma', 0.999),
             ('n_envs', 16),
             ('n_epochs', 4),
             ('n_steps', 1024),
             ('n_timesteps', 1000000.0),
             ('policy', 'MlpPolicy')])
Using 16 environments
Creating test environment
Using cuda device
Log path: logs/ppo/LunarLanderContinuous-v2_7
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | -233     |
| time/              |          |
|    fps             | 5499     |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=24992, episode_reward=-133.85 +/- 25.47
Episode length: 72.20 +/- 9.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.2        |
|    mean_reward          | -134        |
| time/                   |             |
|    total_timesteps      | 24992       |
| train/                  |             |
|    approx_kl            | 0.005659279 |
|    clip_fraction        | 0.0517      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.85       |
|    explained_variance   | -0.000296   |
|    learning_rate        | 0.0003      |
|    loss                 | 2.73e+03    |
|    n_updates            | 4           |
|    policy_gradient_loss | -0.00447    |
|    std                  | 1.01        |
|    value_loss           | 6.79e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | -204     |
| time/              |          |
|    fps             | 3178     |
|    iterations      | 2        |
|    time_elapsed    | 10       |
|    total_timesteps | 32768    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 132        |
|    ep_rew_mean          | -158       |
| time/                   |            |
|    fps                  | 2549       |
|    iterations           | 3          |
|    time_elapsed         | 19         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.00564821 |
|    clip_fraction        | 0.0545     |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.86      |
|    explained_variance   | 0.00315    |
|    learning_rate        | 0.0003     |
|    loss                 | 2.04e+03   |
|    n_updates            | 8          |
|    policy_gradient_loss | -0.00533   |
|    std                  | 1.02       |
|    value_loss           | 3.84e+03   |
----------------------------------------
Eval num_timesteps=49984, episode_reward=-159.03 +/- 90.82
Episode length: 83.40 +/- 7.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 83.4        |
|    mean_reward          | -159        |
| time/                   |             |
|    total_timesteps      | 49984       |
| train/                  |             |
|    approx_kl            | 0.005461364 |
|    clip_fraction        | 0.0422      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.87       |
|    explained_variance   | 0.00611     |
|    learning_rate        | 0.0003      |
|    loss                 | 938         |
|    n_updates            | 12          |
|    policy_gradient_loss | -0.00336    |
|    std                  | 1.01        |
|    value_loss           | 1.89e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 2339     |
|    iterations      | 4        |
|    time_elapsed    | 28       |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=74976, episode_reward=-81.89 +/- 44.56
Episode length: 78.60 +/- 17.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.6         |
|    mean_reward          | -81.9        |
| time/                   |              |
|    total_timesteps      | 74976        |
| train/                  |              |
|    approx_kl            | 0.0068340497 |
|    clip_fraction        | 0.0553       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.86        |
|    explained_variance   | -0.00309     |
|    learning_rate        | 0.0003       |
|    loss                 | 522          |
|    n_updates            | 16           |
|    policy_gradient_loss | 0.000219     |
|    std                  | 1.01         |
|    value_loss           | 1.3e+03      |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    fps             | 2316     |
|    iterations      | 5        |
|    time_elapsed    | 35       |
|    total_timesteps | 81920    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | -93.5       |
| time/                   |             |
|    fps                  | 2301        |
|    iterations           | 6           |
|    time_elapsed         | 42          |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.005632938 |
|    clip_fraction        | 0.0568      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.86       |
|    explained_variance   | -0.00129    |
|    learning_rate        | 0.0003      |
|    loss                 | 417         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00286    |
|    std                  | 1.01        |
|    value_loss           | 1.08e+03    |
-----------------------------------------
Eval num_timesteps=99968, episode_reward=-121.44 +/- 33.57
Episode length: 99.80 +/- 15.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.8         |
|    mean_reward          | -121         |
| time/                   |              |
|    total_timesteps      | 99968        |
| train/                  |              |
|    approx_kl            | 0.0050869463 |
|    clip_fraction        | 0.0486       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.86        |
|    explained_variance   | 0.00278      |
|    learning_rate        | 0.0003       |
|    loss                 | 191          |
|    n_updates            | 24           |
|    policy_gradient_loss | -0.00232     |
|    std                  | 1.01         |
|    value_loss           | 693          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | -66.8    |
| time/              |          |
|    fps             | 2284     |
|    iterations      | 7        |
|    time_elapsed    | 50       |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=124960, episode_reward=-59.87 +/- 77.80
Episode length: 104.20 +/- 22.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 104          |
|    mean_reward          | -59.9        |
| time/                   |              |
|    total_timesteps      | 124960       |
| train/                  |              |
|    approx_kl            | 0.0076860534 |
|    clip_fraction        | 0.0632       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.86        |
|    explained_variance   | -0.00683     |
|    learning_rate        | 0.0003       |
|    loss                 | 177          |
|    n_updates            | 28           |
|    policy_gradient_loss | -0.0026      |
|    std                  | 1.01         |
|    value_loss           | 437          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | -57.7    |
| time/              |          |
|    fps             | 2261     |
|    iterations      | 8        |
|    time_elapsed    | 57       |
|    total_timesteps | 131072   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 129          |
|    ep_rew_mean          | -47.4        |
| time/                   |              |
|    fps                  | 2297         |
|    iterations           | 9            |
|    time_elapsed         | 64           |
|    total_timesteps      | 147456       |
| train/                  |              |
|    approx_kl            | 0.0040048994 |
|    clip_fraction        | 0.0281       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.86        |
|    explained_variance   | -0.00122     |
|    learning_rate        | 0.0003       |
|    loss                 | 174          |
|    n_updates            | 32           |
|    policy_gradient_loss | -0.00261     |
|    std                  | 1.01         |
|    value_loss           | 487          |
------------------------------------------
Eval num_timesteps=149952, episode_reward=-98.30 +/- 26.62
Episode length: 126.80 +/- 27.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 127         |
|    mean_reward          | -98.3       |
| time/                   |             |
|    total_timesteps      | 149952      |
| train/                  |             |
|    approx_kl            | 0.005415641 |
|    clip_fraction        | 0.0397      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.86       |
|    explained_variance   | -2.74e-06   |
|    learning_rate        | 0.0003      |
|    loss                 | 178         |
|    n_updates            | 36          |
|    policy_gradient_loss | -0.00247    |
|    std                  | 1.01        |
|    value_loss           | 321         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | -24.4    |
| time/              |          |
|    fps             | 2211     |
|    iterations      | 10       |
|    time_elapsed    | 74       |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=174944, episode_reward=-108.13 +/- 22.11
Episode length: 155.60 +/- 44.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 156          |
|    mean_reward          | -108         |
| time/                   |              |
|    total_timesteps      | 174944       |
| train/                  |              |
|    approx_kl            | 0.0043199332 |
|    clip_fraction        | 0.0396       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.85        |
|    explained_variance   | -9.06e-06    |
|    learning_rate        | 0.0003       |
|    loss                 | 198          |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00167     |
|    std                  | 1.01         |
|    value_loss           | 423          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | -26.2    |
| time/              |          |
|    fps             | 2240     |
|    iterations      | 11       |
|    time_elapsed    | 80       |
|    total_timesteps | 180224   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 125          |
|    ep_rew_mean          | -22.8        |
| time/                   |              |
|    fps                  | 2277         |
|    iterations           | 12           |
|    time_elapsed         | 86           |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0038525225 |
|    clip_fraction        | 0.0338       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.86        |
|    explained_variance   | -0.000515    |
|    learning_rate        | 0.0003       |
|    loss                 | 179          |
|    n_updates            | 44           |
|    policy_gradient_loss | -0.00079     |
|    std                  | 1.01         |
|    value_loss           | 527          |
------------------------------------------
Eval num_timesteps=199936, episode_reward=-77.23 +/- 40.20
Episode length: 137.40 +/- 32.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 137          |
|    mean_reward          | -77.2        |
| time/                   |              |
|    total_timesteps      | 199936       |
| train/                  |              |
|    approx_kl            | 0.0043804427 |
|    clip_fraction        | 0.045        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.85        |
|    explained_variance   | -4.77e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 208          |
|    n_updates            | 48           |
|    policy_gradient_loss | -0.00211     |
|    std                  | 1            |
|    value_loss           | 524          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | -15.6    |
| time/              |          |
|    fps             | 2264     |
|    iterations      | 13       |
|    time_elapsed    | 94       |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=224928, episode_reward=-70.41 +/- 54.51
Episode length: 183.80 +/- 69.32
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 184        |
|    mean_reward          | -70.4      |
| time/                   |            |
|    total_timesteps      | 224928     |
| train/                  |            |
|    approx_kl            | 0.00442057 |
|    clip_fraction        | 0.0435     |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.85      |
|    explained_variance   | -5.48e-06  |
|    learning_rate        | 0.0003     |
|    loss                 | 369        |
|    n_updates            | 52         |
|    policy_gradient_loss | -0.0013    |
|    std                  | 1          |
|    value_loss           | 601        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | -11.9    |
| time/              |          |
|    fps             | 2260     |
|    iterations      | 14       |
|    time_elapsed    | 101      |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 211         |
|    ep_rew_mean          | 2.14        |
| time/                   |             |
|    fps                  | 2291        |
|    iterations           | 15          |
|    time_elapsed         | 107         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.005154158 |
|    clip_fraction        | 0.0537      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.84       |
|    explained_variance   | 4.71e-06    |
|    learning_rate        | 0.0003      |
|    loss                 | 300         |
|    n_updates            | 56          |
|    policy_gradient_loss | -0.00141    |
|    std                  | 1.01        |
|    value_loss           | 635         |
-----------------------------------------
Eval num_timesteps=249920, episode_reward=58.44 +/- 113.84
Episode length: 248.40 +/- 59.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 248         |
|    mean_reward          | 58.4        |
| time/                   |             |
|    total_timesteps      | 249920      |
| train/                  |             |
|    approx_kl            | 0.005049843 |
|    clip_fraction        | 0.0522      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.85       |
|    explained_variance   | 0.000956    |
|    learning_rate        | 0.0003      |
|    loss                 | 359         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00228    |
|    std                  | 1.01        |
|    value_loss           | 598         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 241      |
|    ep_rew_mean     | 9.23     |
| time/              |          |
|    fps             | 2267     |
|    iterations      | 16       |
|    time_elapsed    | 115      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=274912, episode_reward=147.50 +/- 134.15
Episode length: 298.60 +/- 55.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 299          |
|    mean_reward          | 148          |
| time/                   |              |
|    total_timesteps      | 274912       |
| train/                  |              |
|    approx_kl            | 0.0058997683 |
|    clip_fraction        | 0.0519       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.00314      |
|    learning_rate        | 0.0003       |
|    loss                 | 300          |
|    n_updates            | 64           |
|    policy_gradient_loss | -0.000845    |
|    std                  | 0.997        |
|    value_loss           | 635          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 8.73     |
| time/              |          |
|    fps             | 2249     |
|    iterations      | 17       |
|    time_elapsed    | 123      |
|    total_timesteps | 278528   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 432         |
|    ep_rew_mean          | 13.9        |
| time/                   |             |
|    fps                  | 2274        |
|    iterations           | 18          |
|    time_elapsed         | 129         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.003863364 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.83       |
|    explained_variance   | 0.0166      |
|    learning_rate        | 0.0003      |
|    loss                 | 162         |
|    n_updates            | 68          |
|    policy_gradient_loss | -0.00146    |
|    std                  | 1           |
|    value_loss           | 501         |
-----------------------------------------
Eval num_timesteps=299904, episode_reward=159.32 +/- 135.92
Episode length: 431.60 +/- 287.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 432          |
|    mean_reward          | 159          |
| time/                   |              |
|    total_timesteps      | 299904       |
| train/                  |              |
|    approx_kl            | 0.0031732754 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.433        |
|    learning_rate        | 0.0003       |
|    loss                 | 73.8         |
|    n_updates            | 72           |
|    policy_gradient_loss | -0.00226     |
|    std                  | 0.996        |
|    value_loss           | 193          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 537      |
|    ep_rew_mean     | 20.9     |
| time/              |          |
|    fps             | 2267     |
|    iterations      | 19       |
|    time_elapsed    | 137      |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=324896, episode_reward=178.00 +/- 124.92
Episode length: 335.20 +/- 52.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 335         |
|    mean_reward          | 178         |
| time/                   |             |
|    total_timesteps      | 324896      |
| train/                  |             |
|    approx_kl            | 0.004023699 |
|    clip_fraction        | 0.0302      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.82       |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.4        |
|    n_updates            | 76          |
|    policy_gradient_loss | -0.00241    |
|    std                  | 0.988       |
|    value_loss           | 177         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 626      |
|    ep_rew_mean     | 27.5     |
| time/              |          |
|    fps             | 2231     |
|    iterations      | 20       |
|    time_elapsed    | 146      |
|    total_timesteps | 327680   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 740          |
|    ep_rew_mean          | 38.2         |
| time/                   |              |
|    fps                  | 2228         |
|    iterations           | 21           |
|    time_elapsed         | 154          |
|    total_timesteps      | 344064       |
| train/                  |              |
|    approx_kl            | 0.0039663035 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.81        |
|    explained_variance   | 0.818        |
|    learning_rate        | 0.0003       |
|    loss                 | 89           |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00143     |
|    std                  | 0.987        |
|    value_loss           | 155          |
------------------------------------------
Eval num_timesteps=349888, episode_reward=180.49 +/- 118.10
Episode length: 374.60 +/- 99.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 375          |
|    mean_reward          | 180          |
| time/                   |              |
|    total_timesteps      | 349888       |
| train/                  |              |
|    approx_kl            | 0.0046134708 |
|    clip_fraction        | 0.032        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.81        |
|    explained_variance   | 0.861        |
|    learning_rate        | 0.0003       |
|    loss                 | 37.7         |
|    n_updates            | 84           |
|    policy_gradient_loss | -0.000752    |
|    std                  | 0.983        |
|    value_loss           | 140          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 789      |
|    ep_rew_mean     | 46.8     |
| time/              |          |
|    fps             | 2196     |
|    iterations      | 22       |
|    time_elapsed    | 164      |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=374880, episode_reward=130.03 +/- 133.19
Episode length: 338.40 +/- 38.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 338          |
|    mean_reward          | 130          |
| time/                   |              |
|    total_timesteps      | 374880       |
| train/                  |              |
|    approx_kl            | 0.0052133137 |
|    clip_fraction        | 0.0518       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.79        |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.0003       |
|    loss                 | 18.8         |
|    n_updates            | 88           |
|    policy_gradient_loss | -0.00258     |
|    std                  | 0.974        |
|    value_loss           | 80           |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 832      |
|    ep_rew_mean     | 57.2     |
| time/              |          |
|    fps             | 2187     |
|    iterations      | 23       |
|    time_elapsed    | 172      |
|    total_timesteps | 376832   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 866         |
|    ep_rew_mean          | 67.8        |
| time/                   |             |
|    fps                  | 2177        |
|    iterations           | 24          |
|    time_elapsed         | 180         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.005419878 |
|    clip_fraction        | 0.0593      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.78       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 50.4        |
|    n_updates            | 92          |
|    policy_gradient_loss | -0.00193    |
|    std                  | 0.966       |
|    value_loss           | 84.7        |
-----------------------------------------
Eval num_timesteps=399872, episode_reward=190.38 +/- 111.43
Episode length: 286.40 +/- 59.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 286         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 399872      |
| train/                  |             |
|    approx_kl            | 0.005382113 |
|    clip_fraction        | 0.0572      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.76       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.1        |
|    n_updates            | 96          |
|    policy_gradient_loss | -0.00108    |
|    std                  | 0.956       |
|    value_loss           | 29.6        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 919      |
|    ep_rew_mean     | 75       |
| time/              |          |
|    fps             | 2150     |
|    iterations      | 25       |
|    time_elapsed    | 190      |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=424864, episode_reward=215.73 +/- 25.62
Episode length: 348.80 +/- 26.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 349          |
|    mean_reward          | 216          |
| time/                   |              |
|    total_timesteps      | 424864       |
| train/                  |              |
|    approx_kl            | 0.0048591513 |
|    clip_fraction        | 0.0562       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.74        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.6         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00217     |
|    std                  | 0.959        |
|    value_loss           | 59.4         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 925      |
|    ep_rew_mean     | 79.2     |
| time/              |          |
|    fps             | 2137     |
|    iterations      | 26       |
|    time_elapsed    | 199      |
|    total_timesteps | 425984   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 917          |
|    ep_rew_mean          | 86.2         |
| time/                   |              |
|    fps                  | 2146         |
|    iterations           | 27           |
|    time_elapsed         | 206          |
|    total_timesteps      | 442368       |
| train/                  |              |
|    approx_kl            | 0.0054500382 |
|    clip_fraction        | 0.0473       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.74        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.0003       |
|    loss                 | 28.4         |
|    n_updates            | 104          |
|    policy_gradient_loss | -0.000512    |
|    std                  | 0.949        |
|    value_loss           | 45.7         |
------------------------------------------
Eval num_timesteps=449856, episode_reward=243.20 +/- 35.38
Episode length: 440.00 +/- 133.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 440         |
|    mean_reward          | 243         |
| time/                   |             |
|    total_timesteps      | 449856      |
| train/                  |             |
|    approx_kl            | 0.003808748 |
|    clip_fraction        | 0.0321      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.73       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.9        |
|    n_updates            | 108         |
|    policy_gradient_loss | -0.000502   |
|    std                  | 0.946       |
|    value_loss           | 81.6        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 925      |
|    ep_rew_mean     | 88.8     |
| time/              |          |
|    fps             | 2114     |
|    iterations      | 28       |
|    time_elapsed    | 216      |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=474848, episode_reward=237.18 +/- 52.54
Episode length: 387.20 +/- 129.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 387          |
|    mean_reward          | 237          |
| time/                   |              |
|    total_timesteps      | 474848       |
| train/                  |              |
|    approx_kl            | 0.0044905096 |
|    clip_fraction        | 0.054        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.72        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.3         |
|    n_updates            | 112          |
|    policy_gradient_loss | -0.00186     |
|    std                  | 0.943        |
|    value_loss           | 42.5         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 940      |
|    ep_rew_mean     | 95.5     |
| time/              |          |
|    fps             | 2090     |
|    iterations      | 29       |
|    time_elapsed    | 227      |
|    total_timesteps | 475136   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 928         |
|    ep_rew_mean          | 98.4        |
| time/                   |             |
|    fps                  | 2105        |
|    iterations           | 30          |
|    time_elapsed         | 233         |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.005535678 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.72       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.39        |
|    n_updates            | 116         |
|    policy_gradient_loss | -0.000866   |
|    std                  | 0.943       |
|    value_loss           | 26.2        |
-----------------------------------------
Eval num_timesteps=499840, episode_reward=127.67 +/- 129.57
Episode length: 331.00 +/- 33.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 331          |
|    mean_reward          | 128          |
| time/                   |              |
|    total_timesteps      | 499840       |
| train/                  |              |
|    approx_kl            | 0.0046330206 |
|    clip_fraction        | 0.0562       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.71        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.0003       |
|    loss                 | 108          |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.000762    |
|    std                  | 0.936        |
|    value_loss           | 56.7         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 938      |
|    ep_rew_mean     | 104      |
| time/              |          |
|    fps             | 2083     |
|    iterations      | 31       |
|    time_elapsed    | 243      |
|    total_timesteps | 507904   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 927         |
|    ep_rew_mean          | 107         |
| time/                   |             |
|    fps                  | 2099        |
|    iterations           | 32          |
|    time_elapsed         | 249         |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.004334782 |
|    clip_fraction        | 0.0502      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.7        |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.8        |
|    n_updates            | 124         |
|    policy_gradient_loss | -0.000996   |
|    std                  | 0.933       |
|    value_loss           | 25.7        |
-----------------------------------------
Eval num_timesteps=524832, episode_reward=255.37 +/- 15.51
Episode length: 367.00 +/- 22.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 367          |
|    mean_reward          | 255          |
| time/                   |              |
|    total_timesteps      | 524832       |
| train/                  |              |
|    approx_kl            | 0.0043075555 |
|    clip_fraction        | 0.0374       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.7         |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0003       |
|    loss                 | 30.9         |
|    n_updates            | 128          |
|    policy_gradient_loss | -0.000596    |
|    std                  | 0.932        |
|    value_loss           | 43.4         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 928      |
|    ep_rew_mean     | 111      |
| time/              |          |
|    fps             | 2105     |
|    iterations      | 33       |
|    time_elapsed    | 256      |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=549824, episode_reward=242.56 +/- 31.15
Episode length: 430.20 +/- 77.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 430         |
|    mean_reward          | 243         |
| time/                   |             |
|    total_timesteps      | 549824      |
| train/                  |             |
|    approx_kl            | 0.006331216 |
|    clip_fraction        | 0.0584      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 6           |
|    n_updates            | 132         |
|    policy_gradient_loss | -0.00201    |
|    std                  | 0.94        |
|    value_loss           | 30.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 920      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    fps             | 2101     |
|    iterations      | 34       |
|    time_elapsed    | 265      |
|    total_timesteps | 557056   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 912         |
|    ep_rew_mean          | 112         |
| time/                   |             |
|    fps                  | 2115        |
|    iterations           | 35          |
|    time_elapsed         | 271         |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.004847744 |
|    clip_fraction        | 0.0481      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.7        |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 4.95        |
|    n_updates            | 136         |
|    policy_gradient_loss | -0.000771   |
|    std                  | 0.931       |
|    value_loss           | 37.4        |
-----------------------------------------
Eval num_timesteps=574816, episode_reward=232.98 +/- 5.91
Episode length: 434.60 +/- 147.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 435         |
|    mean_reward          | 233         |
| time/                   |             |
|    total_timesteps      | 574816      |
| train/                  |             |
|    approx_kl            | 0.004384325 |
|    clip_fraction        | 0.0451      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.69       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.12        |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.000744    |
|    std                  | 0.924       |
|    value_loss           | 32.2        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 908      |
|    ep_rew_mean     | 111      |
| time/              |          |
|    fps             | 2115     |
|    iterations      | 36       |
|    time_elapsed    | 278      |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=599808, episode_reward=243.09 +/- 11.08
Episode length: 350.40 +/- 35.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 350          |
|    mean_reward          | 243          |
| time/                   |              |
|    total_timesteps      | 599808       |
| train/                  |              |
|    approx_kl            | 0.0056234724 |
|    clip_fraction        | 0.0615       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.67        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | 28.6         |
|    n_updates            | 144          |
|    policy_gradient_loss | -0.000868    |
|    std                  | 0.921        |
|    value_loss           | 18.5         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 923      |
|    ep_rew_mean     | 115      |
| time/              |          |
|    fps             | 2109     |
|    iterations      | 37       |
|    time_elapsed    | 287      |
|    total_timesteps | 606208   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 954         |
|    ep_rew_mean          | 123         |
| time/                   |             |
|    fps                  | 2100        |
|    iterations           | 38          |
|    time_elapsed         | 296         |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.004815912 |
|    clip_fraction        | 0.0426      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.66       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.02        |
|    n_updates            | 148         |
|    policy_gradient_loss | -3.41e-05   |
|    std                  | 0.911       |
|    value_loss           | 8.35        |
-----------------------------------------
Eval num_timesteps=624800, episode_reward=236.83 +/- 18.57
Episode length: 350.40 +/- 36.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 350         |
|    mean_reward          | 237         |
| time/                   |             |
|    total_timesteps      | 624800      |
| train/                  |             |
|    approx_kl            | 0.004588647 |
|    clip_fraction        | 0.0338      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.3         |
|    n_updates            | 152         |
|    policy_gradient_loss | 0.000145    |
|    std                  | 0.91        |
|    value_loss           | 19.3        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 961      |
|    ep_rew_mean     | 126      |
| time/              |          |
|    fps             | 2105     |
|    iterations      | 39       |
|    time_elapsed    | 303      |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=649792, episode_reward=209.94 +/- 97.74
Episode length: 368.80 +/- 54.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 369         |
|    mean_reward          | 210         |
| time/                   |             |
|    total_timesteps      | 649792      |
| train/                  |             |
|    approx_kl            | 0.004136712 |
|    clip_fraction        | 0.0527      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.6         |
|    n_updates            | 156         |
|    policy_gradient_loss | 0.000372    |
|    std                  | 0.91        |
|    value_loss           | 7.61        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 961      |
|    ep_rew_mean     | 130      |
| time/              |          |
|    fps             | 2111     |
|    iterations      | 40       |
|    time_elapsed    | 310      |
|    total_timesteps | 655360   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 969         |
|    ep_rew_mean          | 133         |
| time/                   |             |
|    fps                  | 2116        |
|    iterations           | 41          |
|    time_elapsed         | 317         |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.004030518 |
|    clip_fraction        | 0.0417      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.85        |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.000851   |
|    std                  | 0.914       |
|    value_loss           | 15.4        |
-----------------------------------------
Eval num_timesteps=674784, episode_reward=223.71 +/- 19.13
Episode length: 416.20 +/- 31.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 416          |
|    mean_reward          | 224          |
| time/                   |              |
|    total_timesteps      | 674784       |
| train/                  |              |
|    approx_kl            | 0.0033709793 |
|    clip_fraction        | 0.0289       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.64        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.48         |
|    n_updates            | 164          |
|    policy_gradient_loss | 0.000242     |
|    std                  | 0.907        |
|    value_loss           | 16.5         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 977      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 2106     |
|    iterations      | 42       |
|    time_elapsed    | 326      |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=699776, episode_reward=238.45 +/- 20.14
Episode length: 353.60 +/- 28.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 354          |
|    mean_reward          | 238          |
| time/                   |              |
|    total_timesteps      | 699776       |
| train/                  |              |
|    approx_kl            | 0.0047959615 |
|    clip_fraction        | 0.0518       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.64        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.82         |
|    n_updates            | 168          |
|    policy_gradient_loss | 0.000591     |
|    std                  | 0.906        |
|    value_loss           | 7.49         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 977      |
|    ep_rew_mean     | 138      |
| time/              |          |
|    fps             | 2077     |
|    iterations      | 43       |
|    time_elapsed    | 339      |
|    total_timesteps | 704512   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 984         |
|    ep_rew_mean          | 140         |
| time/                   |             |
|    fps                  | 2071        |
|    iterations           | 44          |
|    time_elapsed         | 348         |
|    total_timesteps      | 720896      |
| train/                  |             |
|    approx_kl            | 0.004188396 |
|    clip_fraction        | 0.0433      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.63       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.76        |
|    n_updates            | 172         |
|    policy_gradient_loss | 0.000216    |
|    std                  | 0.904       |
|    value_loss           | 5.04        |
-----------------------------------------
Eval num_timesteps=724768, episode_reward=246.37 +/- 21.06
Episode length: 382.00 +/- 39.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 382          |
|    mean_reward          | 246          |
| time/                   |              |
|    total_timesteps      | 724768       |
| train/                  |              |
|    approx_kl            | 0.0046440503 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.63        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.23         |
|    n_updates            | 176          |
|    policy_gradient_loss | 0.000204     |
|    std                  | 0.907        |
|    value_loss           | 7.35         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 140      |
| time/              |          |
|    fps             | 2073     |
|    iterations      | 45       |
|    time_elapsed    | 355      |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=749760, episode_reward=238.77 +/- 11.59
Episode length: 416.60 +/- 38.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 239          |
| time/                   |              |
|    total_timesteps      | 749760       |
| train/                  |              |
|    approx_kl            | 0.0045059104 |
|    clip_fraction        | 0.0474       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.65        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.87         |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.000282    |
|    std                  | 0.914        |
|    value_loss           | 5.76         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 141      |
| time/              |          |
|    fps             | 2078     |
|    iterations      | 46       |
|    time_elapsed    | 362      |
|    total_timesteps | 753664   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 143          |
| time/                   |              |
|    fps                  | 2091         |
|    iterations           | 47           |
|    time_elapsed         | 368          |
|    total_timesteps      | 770048       |
| train/                  |              |
|    approx_kl            | 0.0048520532 |
|    clip_fraction        | 0.0479       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.65        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.73         |
|    n_updates            | 184          |
|    policy_gradient_loss | -0.000532    |
|    std                  | 0.912        |
|    value_loss           | 6.29         |
------------------------------------------
Eval num_timesteps=774752, episode_reward=235.06 +/- 16.04
Episode length: 358.80 +/- 23.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 359         |
|    mean_reward          | 235         |
| time/                   |             |
|    total_timesteps      | 774752      |
| train/                  |             |
|    approx_kl            | 0.004334368 |
|    clip_fraction        | 0.042       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.66       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48        |
|    n_updates            | 188         |
|    policy_gradient_loss | 0.000755    |
|    std                  | 0.915       |
|    value_loss           | 6.21        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 142      |
| time/              |          |
|    fps             | 2096     |
|    iterations      | 48       |
|    time_elapsed    | 375      |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=799744, episode_reward=262.13 +/- 14.46
Episode length: 368.40 +/- 18.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 368          |
|    mean_reward          | 262          |
| time/                   |              |
|    total_timesteps      | 799744       |
| train/                  |              |
|    approx_kl            | 0.0030705675 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.65        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0003       |
|    loss                 | 18.9         |
|    n_updates            | 192          |
|    policy_gradient_loss | -9.91e-05    |
|    std                  | 0.91         |
|    value_loss           | 29.1         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    fps             | 2102     |
|    iterations      | 49       |
|    time_elapsed    | 381      |
|    total_timesteps | 802816   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 984          |
|    ep_rew_mean          | 147          |
| time/                   |              |
|    fps                  | 2113         |
|    iterations           | 50           |
|    time_elapsed         | 387          |
|    total_timesteps      | 819200       |
| train/                  |              |
|    approx_kl            | 0.0045587057 |
|    clip_fraction        | 0.0481       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.62        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.96         |
|    n_updates            | 196          |
|    policy_gradient_loss | -6.29e-05    |
|    std                  | 0.898        |
|    value_loss           | 4.34         |
------------------------------------------
Eval num_timesteps=824736, episode_reward=249.27 +/- 20.35
Episode length: 324.40 +/- 14.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 324         |
|    mean_reward          | 249         |
| time/                   |             |
|    total_timesteps      | 824736      |
| train/                  |             |
|    approx_kl            | 0.005275052 |
|    clip_fraction        | 0.0514      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.62       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.48        |
|    n_updates            | 200         |
|    policy_gradient_loss | 0.000301    |
|    std                  | 0.898       |
|    value_loss           | 4.31        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 147      |
| time/              |          |
|    fps             | 2116     |
|    iterations      | 51       |
|    time_elapsed    | 394      |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=849728, episode_reward=265.57 +/- 4.59
Episode length: 361.60 +/- 70.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 362         |
|    mean_reward          | 266         |
| time/                   |             |
|    total_timesteps      | 849728      |
| train/                  |             |
|    approx_kl            | 0.005119227 |
|    clip_fraction        | 0.0457      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.62       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.27        |
|    n_updates            | 204         |
|    policy_gradient_loss | 0.000598    |
|    std                  | 0.9         |
|    value_loss           | 6.42        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    fps             | 2122     |
|    iterations      | 52       |
|    time_elapsed    | 401      |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 984         |
|    ep_rew_mean          | 149         |
| time/                   |             |
|    fps                  | 2132        |
|    iterations           | 53          |
|    time_elapsed         | 407         |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.005343006 |
|    clip_fraction        | 0.0445      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.61       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.57        |
|    n_updates            | 208         |
|    policy_gradient_loss | -0.00047    |
|    std                  | 0.896       |
|    value_loss           | 5.34        |
-----------------------------------------
Eval num_timesteps=874720, episode_reward=261.35 +/- 19.50
Episode length: 337.60 +/- 13.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 338          |
|    mean_reward          | 261          |
| time/                   |              |
|    total_timesteps      | 874720       |
| train/                  |              |
|    approx_kl            | 0.0047047245 |
|    clip_fraction        | 0.0489       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.61        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.67         |
|    n_updates            | 212          |
|    policy_gradient_loss | 0.000439     |
|    std                  | 0.889        |
|    value_loss           | 3.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    fps             | 2125     |
|    iterations      | 54       |
|    time_elapsed    | 416      |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=899712, episode_reward=271.80 +/- 27.68
Episode length: 317.60 +/- 14.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 318          |
|    mean_reward          | 272          |
| time/                   |              |
|    total_timesteps      | 899712       |
| train/                  |              |
|    approx_kl            | 0.0032094694 |
|    clip_fraction        | 0.0439       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.59        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.1          |
|    n_updates            | 216          |
|    policy_gradient_loss | -0.00018     |
|    std                  | 0.88         |
|    value_loss           | 17.5         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 977      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    fps             | 2124     |
|    iterations      | 55       |
|    time_elapsed    | 424      |
|    total_timesteps | 901120   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 977          |
|    ep_rew_mean          | 147          |
| time/                   |              |
|    fps                  | 2132         |
|    iterations           | 56           |
|    time_elapsed         | 430          |
|    total_timesteps      | 917504       |
| train/                  |              |
|    approx_kl            | 0.0033928244 |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.58        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.86         |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00102     |
|    std                  | 0.876        |
|    value_loss           | 24.9         |
------------------------------------------
Eval num_timesteps=924704, episode_reward=263.49 +/- 18.16
Episode length: 374.40 +/- 88.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 374         |
|    mean_reward          | 263         |
| time/                   |             |
|    total_timesteps      | 924704      |
| train/                  |             |
|    approx_kl            | 0.005677187 |
|    clip_fraction        | 0.056       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.56       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.695       |
|    n_updates            | 224         |
|    policy_gradient_loss | 7.66e-05    |
|    std                  | 0.872       |
|    value_loss           | 3.39        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 977      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    fps             | 2136     |
|    iterations      | 57       |
|    time_elapsed    | 437      |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=949696, episode_reward=265.07 +/- 28.10
Episode length: 314.60 +/- 12.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 315          |
|    mean_reward          | 265          |
| time/                   |              |
|    total_timesteps      | 949696       |
| train/                  |              |
|    approx_kl            | 0.0054137073 |
|    clip_fraction        | 0.0593       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.56        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.519        |
|    n_updates            | 228          |
|    policy_gradient_loss | -0.000482    |
|    std                  | 0.871        |
|    value_loss           | 2.96         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 977      |
|    ep_rew_mean     | 147      |
| time/              |          |
|    fps             | 2140     |
|    iterations      | 58       |
|    time_elapsed    | 443      |
|    total_timesteps | 950272   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 969          |
|    ep_rew_mean          | 149          |
| time/                   |              |
|    fps                  | 2148         |
|    iterations           | 59           |
|    time_elapsed         | 449          |
|    total_timesteps      | 966656       |
| train/                  |              |
|    approx_kl            | 0.0044810968 |
|    clip_fraction        | 0.0494       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.55        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.59         |
|    n_updates            | 232          |
|    policy_gradient_loss | -0.000744    |
|    std                  | 0.866        |
|    value_loss           | 3.15         |
------------------------------------------
Eval num_timesteps=974688, episode_reward=278.67 +/- 14.22
Episode length: 281.00 +/- 23.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 974688      |
| train/                  |             |
|    approx_kl            | 0.004833472 |
|    clip_fraction        | 0.0384      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 11          |
|    n_updates            | 236         |
|    policy_gradient_loss | 3.17e-05    |
|    std                  | 0.864       |
|    value_loss           | 24.7        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    fps             | 2139     |
|    iterations      | 60       |
|    time_elapsed    | 459      |
|    total_timesteps | 983040   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 992          |
|    ep_rew_mean          | 155          |
| time/                   |              |
|    fps                  | 2138         |
|    iterations           | 61           |
|    time_elapsed         | 467          |
|    total_timesteps      | 999424       |
| train/                  |              |
|    approx_kl            | 0.0040015816 |
|    clip_fraction        | 0.0495       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.54        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.58         |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.000196    |
|    std                  | 0.864        |
|    value_loss           | 2.33         |
------------------------------------------
Eval num_timesteps=999680, episode_reward=276.52 +/- 14.48
Episode length: 305.20 +/- 14.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 305          |
|    mean_reward          | 277          |
| time/                   |              |
|    total_timesteps      | 999680       |
| train/                  |              |
|    approx_kl            | 0.0049586715 |
|    clip_fraction        | 0.0579       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.55        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.426        |
|    n_updates            | 244          |
|    policy_gradient_loss | -0.000601    |
|    std                  | 0.868        |
|    value_loss           | 2.45         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 992      |
|    ep_rew_mean     | 156      |
| time/              |          |
|    fps             | 2141     |
|    iterations      | 62       |
|    time_elapsed    | 474      |
|    total_timesteps | 1015808  |
---------------------------------
Saving to logs/ppo/LunarLanderContinuous-v2_7
